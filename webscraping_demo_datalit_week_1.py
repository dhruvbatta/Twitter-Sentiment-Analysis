# -*- coding: utf-8 -*-
"""WebScraping_Demo_DataLit_Week_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19fp9tajYLoBARJaTEmGa-d505TIUQR_q

## Webscraping Demo

### School of AI - DataLit Week 1

#### Any mistakes made here are the sole property of I, Carson Bentley

### Just for fun, let's collect some movie posters from imdb.com

First, import the necessary libraries. 

In google-colab these come preinstalled, but they are not part of the python standard library as of the writing of this article.
"""

from bs4 import BeautifulSoup

import requests

def request_webpage(url):
  res = requests.get(url)
  try:
    res.raise_for_status()
  except Exception as exc:
    print('There was a problem with the request: %s' % (exc))
  return res

"""Next, create an object to store the webpage locally"""

coming_soon_page = request_webpage('https://www.imdb.com/movies-coming-soon/2019-01/')

"""Let's take a look at what we got back"""

coming_soon_page.text

"""The 'prettify' function will help to make this a bit more human readable"""

coming_soon_soup = BeautifulSoup(coming_soon_page.text)
print(coming_soon_soup.prettify())

"""Next, let's find the images.

You can locate an HTML element by right clicking on the webpage and selecting 'inspect'.

The code below is looking for an element like this: 

< div class="list detail" >... (content)...< /div >
"""

details = coming_soon_soup.find('div', attrs = {'class': 'list detail'})
image_details = details.find_all('img')

#this list comprehension will get all the 'src' data (urls) of the posters,
#while filter out icons for ratings
image_list = [x['src'] for x in image_details if 'poster' in x['class']]
image_list

"""We can get the full size image URLs by removing everything between '_V1_' and '.jpg'"""

image_url = image_list[0]
slice_index = image_url.find('_V1_')
full_size_image_url = image_url[:slice_index] + '_V1_.jpg'

full_size_image_url

img_res = request_webpage(full_size_image_url)

imageFile = open('MoviePoster0'+'.jpg', 'wb')
for chunk in img_res.iter_content(100000):
  imageFile.write(chunk)
imageFile.close()

"""You can find files (in colab) by clicking the '>' icon on the top-left side of the screen. You may need to refresh.

## Challenge 1: 

Find the names of the movies (for this month). 

Put them in a list.
"""



"""## (Solution)"""

## hide me!!!!

# these two lines are just a reminder, since we've already run them above:
# details = coming_soon_soup.find('div', attrs = {'class': 'list detail'})
# image_details = details.find_all('img')

name_list = [x['alt'] for x in image_details if 'poster' in x['class']]
name_list

"""## Challenge 2: 

Collect all of the movie posters (for this month).

Put them in a folder.
"""

# hint:
import os



"""## (Solution)"""

## hide me!!!!

current_date = '2019-01'
try:
  os.makedirs(current_date)
except:
  print('failed gracefully, you probably already made the folder')

for i in range(len(image_list)):
  image_url = image_list[i]
  slice_index = image_url.find('_V1_')
  full_size_image_url = image_url[:slice_index] + '_V1_.jpg'
  img_res = request_webpage(full_size_image_url)
  try:
    imageFile = open(os.path.join(current_date, name_list[i] + '.jpg'), 'wb')
    for chunk in img_res.iter_content(100000):
      imageFile.write(chunk)
    imageFile.close()
  except Exception as exc:
    print('There was a problem with writing the file for %s: %s' % (name_list[i], exc))
    
print('All Finished')

"""## Challenge 3:

Collect a year's worth of movie posters, placing them in folders by month.


(feel free to explore the past, as the interface allows you to go back as far as January 2011)
"""



"""## (Solution)"""

## hide me!!!!

def collect_media_info(date):
  url = 'https://www.imdb.com/movies-coming-soon/' + date + '/'
  soup = BeautifulSoup(request_webpage(url).text)
  
  details = soup.find('div', attrs = {'class': 'list detail'})
  image_details = details.find_all('img')
  
  image_list = [x['src'] for x in image_details if 'poster' in x['class']]
  name_list = [x['alt'] for x in image_details if 'poster' in x['class']]
  return (image_list, name_list)

def download_month_of_posters(images, names, date):
  try:
    os.makedirs(date)
  except:
    print('failed gracefully, you probably already made the folder')
    
  for i in range(len(images)):
    image_url = images[i]
    slice_index = image_url.find('_V1_')
    full_size_image_url = image_url[:slice_index] + '_V1_.jpg'
    img_res = request_webpage(full_size_image_url)
    name = names[i]
    if ('/' in name): # because file names can't have a slash
      name = name.replace('/', '-') 
    try:
      imageFile = open(os.path.join(date, name + '.jpg'), 'wb')
      for chunk in img_res.iter_content(100000):
        imageFile.write(chunk)
      imageFile.close()
    except Exception as exc:
      print('There was a problem with writing the file for %s: %s' % (names[i], exc))
    
  print('All Finished with %s' % (date))

# month numbers for the URLs
month_nums = [str(x+1).zfill(2) for x in range(12)]
month_nums

year = '2019'
for ii in month_nums:
  date = year + '-' + ii
  images, names = collect_media_info(date)
  download_month_of_posters(images, names, date)

"""## Challenge 4:

Zip all the files you have collected in order to download them on your local machine (without a million clicks)

See this article for how to go about zipping: https://www.geeksforgeeks.org/working-zip-files-python/
"""

# hint 1:
from zipfile import ZipFile

# hint 2:
# code for deleting a directory
# in case you make a mistake and want to clean up: 
!rm -rf '2018-01' # '2018-01' is a folder of files to delete



"""## (Solution)"""

## hide me!!!!

def get_file_paths(directory): 
  file_paths = []
  files = os.listdir(directory)
  for filename in files: 
    filepath = os.path.join(directory, filename)
    file_paths.append(filepath)
  return file_paths

year = '2019'
cwd_file_paths = []
for i in range(12):
  date = year + '-' + str(i+1).zfill(2)
  cwd_file_paths += get_file_paths(date)

print('The following files will be zipped:') 
for file_name in cwd_file_paths: 
  print(file_name) 
  
with ZipFile(year+'-movie-posters.zip','w') as zip: 
  for file in cwd_file_paths:
    zip.write(file)
  
print('All files zipped successfully!')